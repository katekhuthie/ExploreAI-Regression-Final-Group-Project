{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59554160-394c-44e8-80eb-fadadc0db60c",
   "metadata": {},
   "source": [
    "# Regression Group Project CO2 Emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a754712-9116-4efb-a6a2-e7386f992ec3",
   "metadata": {},
   "source": [
    "<img src=\"https://images.propertycasualty360.com/contrib/content/uploads/sites/414/2022/04/Pollution-Solution-Image_0422-NU.jpg\" alt=\"Pollution Solution\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89cd7d-74c2-4a00-9d9e-e2b0c0d733f4",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "##### Khuthadzo Tshikovhele\n",
    "##### Dembe Tsiwana\n",
    "##### Keamogetswe Peterson\n",
    "##### Aston Greeves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d347f54-b10b-452d-8c21-10f262db244f",
   "metadata": {},
   "source": [
    "The following notebook demonstrates models that were built using the CO2 Emissions data where we will be predicting emmissions per area using the data provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60dd22a-e997-44b0-9f96-f1a205e236ec",
   "metadata": {},
   "source": [
    "## Loading of Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46a32e-2482-4e95-9788-505cb1b9219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456884a-45ca-468e-bfa0-98c74b4bb669",
   "metadata": {},
   "source": [
    "#### Packages\n",
    "\n",
    "These packages collectively provide a powerful toolkit for handling, analyzing, and visualizing data.\n",
    "1. Pandas Why we need it: Pandas is essential for data manipulation and analysis. It provides data structures like DataFrames which are perfect for handling tabular data. What it is used for: Reading Data: Load the CSV file into a DataFrame.\n",
    "2. Data Cleaning: Handle missing values, filter data, and perform transformations. Data Analysis: Compute statistics and aggregate data. Data Visualization: Plot data directly from DataFrames.\n",
    "3. NumPy Why we need it: NumPy is fundamental for numerical operations in Python. It provides support for arrays and matrices, along with a collection of mathematical functions to operate on these arrays. What it is used for: Array Operations: Perform element-wise operations on data. Mathematical Functions: Use built-in functions for complex mathematical operations. Data Conversion: Convert data types and structures for compatibility with other libraries.\n",
    "4. Matplotlib/Seaborn Why we need it: These libraries are crucial for data visualization. They help in creating static, animated, and interactive visualizations in Python. What it is used for: Plotting: Create various types of plots like line charts, bar charts, histograms, etc. Customization: Customize the appearance of plots (titles, labels, colors). Statistical Visualization: Seaborn provides high-level interfaces for drawing attractive statistical graphics.\n",
    "5. SciPy Why we need it: SciPy builds on NumPy and provides additional functionality for scientific and technical computing. What it is used for: Statistical Analysis: Perform statistical tests and computations. Optimization: Solve optimization problems. Signal Processing: Process and analyze signals.\n",
    "6. Scikit-learn Why we need it: Scikit-learn is essential for machine learning. It provides simple and efficient tools for data mining and data analysis. What it is used for: Preprocessing: Scale and transform data. Modeling: Build and train machine learning models. Evaluation: Evaluate model performance using metrics.\n",
    "7. Jupyter Notebook Why we need it: Jupyter Notebook is an interactive computing environment that enables users to create and share documents that contain live code, equations, visualizations, and narrative text. What it is used for: Interactive Coding: Write and execute code in an interactive environment. Documentation: Combine code with rich text elements like paragraphs, equations, and visualizations. Data Exploration: Explore data and visualize results in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78354735-2c55-4367-9f4f-a50e2e1a1a62",
   "metadata": {},
   "source": [
    "## Data Collection and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7aa45-f54e-420b-9826-47bf1070c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('co2_emissions_from_agri.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e4ced-83d1-42da-afe1-620a8c31db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "print(\"Initial data:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80752427-3b68-4824-97b5-b4348aecc4a5",
   "metadata": {},
   "source": [
    "This code displays the first few rows of a dataframe.\n",
    "\n",
    "1. Load the Data: The code assumes that the dataset co2_emissions_from_agri 1.csv has already been loaded into a dataframe named data.\n",
    "\n",
    "2. Print Initial Data:print(\"Initial data:\"): This line prints the string \"Initial data:\" to the console. It's a simple way to label the output that follows. \n",
    "3. print(data.head()): This line prints the first few rows of the dataframe data. The head() method in pandas, by default, returns the first five rows of the dataframe. This is useful for quickly inspecting the structure and contents of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f077978-6ed0-4e1d-bb5e-d8901fa21954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names of the dataframe\n",
    "print(\"\\nColumn names in the dataframe:\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea563ee-3f65-45bd-9443-22b4b5e6bc14",
   "metadata": {},
   "source": [
    "This code snippet is designed to display the column names of a dataframe. \n",
    "Here's a detailed explanation of what it does:\n",
    "\n",
    "1. Print Column Names Label:\n",
    "print(\"\\nColumn names in the dataframe:\"): This line prints the string \"Column names in the dataframe:\" to the console. The \\n at the beginning adds a newline for better readability. Print Column Names:\n",
    "print(data.columns): This line prints the names of all the columns in the dataframe data. The columns attribute of a pandas dataframe returns an Index object containing the column names.This output shows the names of all the columns in the dataframe, giving you a quick overview of the structure of the dataset. This output shows the names of all the columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9692b-af6c-45b5-8280-ca2b9e99004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with missing values\n",
    "data_cleaned = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715692c7-dbe4-4a82-9bea-1719ed2398da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the cleaned dataframe\n",
    "print(\"\\nData after dropping rows with missing values:\")\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409eea53-6304-4681-a008-2a32344a16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'CO2_Emissions' is in the columns\n",
    "if 'total_emission' in data_cleaned.columns:\n",
    "    # Filter the data for a specific condition (e.g., CO2 emissions greater than a certain value)\n",
    "    filtered_data = data_cleaned[data_cleaned['total_emission'] > 1000]\n",
    "       \n",
    "# Display the first few rows of the filtered dataframe\n",
    "    print(\"\\nFiltered data (total_emission > 1000):\")\n",
    "    print(filtered_data.head())\n",
    "\n",
    "# Save the cleaned and filtered data to a new CSV file\n",
    "    filtered_data.to_csv('cleaned_filtered_co2_emissions.csv', index=False)\n",
    "\n",
    "    print(\"\\nCleaned and filtered data saved to 'cleaned_filtered_co2_emissions.csv'\")\n",
    "else:\n",
    "    print(\"\\nColumn 'CO2_Emissions' not found in the dataframe. Please check the column names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d61b9b-aa73-4653-ae8d-641a643e037a",
   "metadata": {},
   "source": [
    "This code snippet checks if the column 'total_emission' exists in the dataframe data_cleaned. If it does, it filters the dataframe to include only rows where 'total_emission' is greater than 1000, displays the first few rows of the filtered data, and saves this filtered data to a new CSV file named 'cleaned_filtered_co2_emissions.csv'. If the column is not found, it prints a message indicating that 'total_emission' is not in the dataframe and suggests checking the column names. This process ensures specific data is filtered and saved, providing clear feedback to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a40387-22b6-4e2b-8b32-2c9fcd6a45d1",
   "metadata": {},
   "source": [
    "**Handling Duplicate rows and removing them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a424753-0f38-493e-b927-d90583b401b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "data_cleaned = data_cleaned.drop_duplicates()\n",
    "\n",
    "# Display the first few rows of the dataframe after removing duplicates\n",
    "print(\"\\nData after removing duplicate rows:\")\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d243158-0bb1-48ff-913d-a9988bb4ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types if necessary (e.g., converting a column to numeric)\n",
    "# Assuming 'Year' column exists and needs to be converted to integer\n",
    "if 'Year' in data_cleaned.columns:\n",
    "    data_cleaned['Year'] = pd.to_numeric(data_cleaned['Year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9979e3-c8f0-4952-8b13-b544795a7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types of the dataframe\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(data_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57e6cf-1898-4a91-b162-0ed7bbf0aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for consistency (e.g., converting column names to lowercase)\n",
    "data_cleaned.columns = [col.lower() for col in data_cleaned.columns]\n",
    "\n",
    "# Display the column names after renaming\n",
    "print(\"\\nColumn names after renaming:\")\n",
    "print(data_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55e03f-2825-403a-9e51-13f75cce9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "data_cleaned.to_csv('cleaned_co2_emissions.csv', index=False)\n",
    "\n",
    "print(\"\\nCleaned data saved to 'cleaned_co2_emissions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c52418-c36e-4287-98ad-78062eb0e2e2",
   "metadata": {},
   "source": [
    "**Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65b97a-0754-43e7-aa2d-fad98086fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for outlier removal\n",
    "numeric_data = data.select_dtypes(include=['number'])\n",
    "\n",
    "# Remove outliers using the IQR method\n",
    "Q1 = numeric_data.quantile(0.25)\n",
    "Q3 = numeric_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out the outliers\n",
    "data_no_outliers = numeric_data[~((numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Display the first few rows of the dataframe after removing outliers\n",
    "print(\"\\nData after removing outliers:\")\n",
    "print(data_no_outliers.head())\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "data_no_outliers.to_csv('data_no_outliers.csv', index=False)\n",
    "\n",
    "print(\"\\nData without outliers saved to 'data_no_outliers.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4becce42-c74d-4b6b-87d5-4365332d7594",
   "metadata": {},
   "source": [
    "This code snippet selects numeric columns from a dataframe, removes outliers using the Interquartile Range (IQR) method, displays the first few rows of the cleaned dataframe, and saves the cleaned data to a new CSV file. It first selects numeric columns with data.select_dtypes(include=['number']), calculates the IQR for each column, and filters out rows with values outside the range [Q1 - 1.5 * IQR, Q3 + 1.5 * IQR]. It then prints the cleaned data and saves it to 'data_no_outliers.csv', confirming the save with a message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbad77d-53ab-41a8-9703-4638b1a62c81",
   "metadata": {},
   "source": [
    "**Inputing Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee716e2b-7723-4c49-8747-92890c8df650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean of the numeric columns\n",
    "data_filled = data.fillna(data.mean(numeric_only=True))\n",
    "\n",
    "# Display the first few rows of the dataframe after filling missing values\n",
    "print(\"\\nData after filling missing values with mean:\")\n",
    "print(data_filled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8862cef-dd4f-4e18-be9c-8938befa8052",
   "metadata": {},
   "source": [
    "**Normalizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abfb67c-7840-41d1-bb60-a3ba4f81e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "# Sample data provided by the user\n",
    "data_dict = {\n",
    "    \"Area\": [\"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\"],\n",
    "    \"Year\": [1990, 1991, 1992, 1993, 1994],\n",
    "    \"Savanna fires\": [14.7237, 14.7237, 14.7237, 14.7237, 14.7237],\n",
    "    \"Forest fires\": [0.0557, 0.0557, 0.0557, 0.0557, 0.0557],\n",
    "    \"Crop Residues\": [205.6077, 209.4971, 196.5341, 230.8175, 242.0494],\n",
    "    \"Rice Cultivation\": [686.00, 678.16, 686.00, 686.00, 705.60],\n",
    "    \"Drained organic soils (CO2)\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"Pesticides Manufacturing\": [11.807483, 11.712073, 11.712073, 11.712073, 11.712073],\n",
    "    \"Food Transport\": [63.1152, 61.2125, 53.3170, 54.3617, 53.9874],\n",
    "    \"Forestland\": [-2388.803, -2388.803, -2388.803, -2388.803, -2388.803],\n",
    "    \"Manure Management\": [319.1763, 342.3079, 349.1224, 352.2947, 367.6784],\n",
    "    \"Fires in organic soils\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"Fires in humid tropical forests\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"On-farm energy use\": [None] * 5,\n",
    "    \"Rural population\": [9655167, 10230490, 10995568, 11858090, 12690115],\n",
    "    \"Urban population\": [2593947, 2763167, 2985663, 3237009, 3482604],\n",
    "    \"Total Population - Male\": [5348387.0, 5372959.0, 6028494.0, 7003641.0, 7733458.0],\n",
    "    \"Total Population - Female\": [5346409.0, 5372208.0, 6028939.0, 7000119.0, 7722096.0],\n",
    "    \"total_emission\": [2198.963539, 2323.876629, 2356.304229, 2368.470529, 2500.768729],\n",
    "    \"Average Temperature °C\": [0.536167, 0.020667, -0.259583, 0.101917, 0.372250]\n",
    "}\n",
    " \n",
    "# Create DataFrame\n",
    "data = pd.DataFrame(data_dict)\n",
    " \n",
    "# Display the initial data\n",
    "print(\"Initial data:\")\n",
    "print(data.head())\n",
    " \n",
    "# Fill missing values with the mean of the numeric columns\n",
    "data_filled = data.fillna(data.mean(numeric_only=True))\n",
    " \n",
    "# Normalize the numeric columns\n",
    "numeric_columns = data_filled.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = pd.DataFrame(scaler.fit_transform(data_filled[numeric_columns]), columns=numeric_columns)\n",
    " \n",
    "# Combine the normalized numeric columns with the non-numeric columns\n",
    "data_normalized = pd.concat([data_filled.select_dtypes(exclude=['float64', 'int64']).reset_index(drop=True), data_normalized], axis=1)\n",
    " \n",
    "# Display the first few rows of the normalized dataframe\n",
    "print(\"\\nNormalized data:\")\n",
    "print(data_normalized.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d81878-b4c9-4d24-b534-8235aba98895",
   "metadata": {},
   "source": [
    "This code snippet performs data preprocessing tasks, including creating a dataframe, filling missing values, normalizing numeric columns, and displaying the processed data. It imports pandas and MinMaxScaler, creates a dataframe from a dictionary, fills missing values with the mean of numeric columns, normalizes numeric columns using MinMaxScaler, combines normalized numeric columns with non-numeric columns, and displays the first few rows of the normalized dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14013e00-261a-48df-a781-38e077ab66c5",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97102a1-5849-48ea-83c5-aa7cc76643ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['total_emission'], kde=True)\n",
    "plt.title('Distribution of Total Emissions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7590d1-b105-4899-9c52-7e539c50b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1469f61f-12f8-44bb-89f3-2734bd2fcc2c",
   "metadata": {},
   "source": [
    "The pairplots indicate correllations between a number of variables particularly total_emission and variables relating to the food production like food processing, food transport, etc.\n",
    "These correlations indicate that in areas where it is more industrialized total_emission is higher, e.g. when food processing increases then so does emission showing a positive correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba403ba7-9e1d-403a-ae65-5cee0f984ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Plotly Express\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('cleaned_co2_emissions.csv')\n",
    "\n",
    "# Scatter plot\n",
    "fig = px.scatter(data, x='Year', y='total_emission', color='Area')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917f8c7a-8329-4cb7-9c3c-2a87cea5f2ef",
   "metadata": {},
   "source": [
    "The scatter plot shows the relationship between total_emission and year where as the years progress the total emission increases in areas where the total_population is higher. This could be indicative of developments within the areas where production is increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb092f-10ae-4f61-b875-c6c0760372ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('cleaned_co2_emissions.csv')\n",
    "\n",
    "# Generate some data\n",
    "data = np.random.rand(10, 12)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 8))  # Adjust the width and height as needed\n",
    "sns.heatmap(data, annot=True, cmap='viridis')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c374c54-689d-4c86-97b0-64426d13dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('cleaned_co2_emissions.csv')\n",
    "\n",
    "# Group the data by 'Area' and calculate the total emissions for each area\n",
    "total_emissions_by_area = data.groupby('Area')['total_emission'].sum()\n",
    "\n",
    "# Get the top 10 and bottom 10 areas with the highest and lowest emissions\n",
    "top10_emissions_areas = total_emissions_by_area.nlargest(10)\n",
    "bottom10_emissions_areas = total_emissions_by_area.nsmallest(10)\n",
    "\n",
    "# Plot the top 10 areas with the highest emissions as a horizontal bar graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "top10_emissions_areas.plot(kind='barh', color='blue')\n",
    "plt.title('Areas with the Highest CO2 Emissions')\n",
    "plt.xlabel('Total Emissions')\n",
    "plt.ylabel('Area')\n",
    "plt.show()\n",
    "\n",
    "# Plot the bottom 10 areas with the lowest emissions as a horizontal bar graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "bottom10_emissions_areas.plot(kind='barh', color='green')\n",
    "plt.title('Areas with the Lowest CO2 Emissions')\n",
    "plt.xlabel('Total Emissions')\n",
    "plt.ylabel('Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85d927-2c46-4237-a094-20ebc9aa872c",
   "metadata": {},
   "source": [
    "Areas with higher urban population numbers generally have higher CO2 emissions this can be attributted to the increase in the need for food production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88cf9d-402c-4419-a8c3-23f4b657f069",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd802d3-31df-48ce-b204-564c3b8b1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('cleaned_co2_emissions.csv')\n",
    "\n",
    "# Display the initial data\n",
    "print(\"Initial data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean of the numeric columns\n",
    "df.fillna(df.mean(numeric_only=True), inplace=True)\n",
    "\n",
    "# One-hot encode the 'Area' column\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "area_encoded = encoder.fit_transform(df[['Area']])\n",
    "area_encoded_df = pd.DataFrame(area_encoded, columns=encoder.get_feature_names_out(['Area']))\n",
    "\n",
    "# Combine the encoded 'Area' column with the rest of the dataframe\n",
    "df = pd.concat([df.drop('Area', axis=1), area_encoded_df], axis=1)\n",
    "\n",
    "# Normalize the numeric columns\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
    "\n",
    "# Display the first few rows of the normalized dataframe\n",
    "print(\"\\nNormalized data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['total_emission'], kde=True)\n",
    "plt.title('Distribution of Total Emissions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d856deb1-3fe9-4700-b1ff-6aefc649aeee",
   "metadata": {},
   "source": [
    "This code is a comprehensive data preprocessing, including handling missing values, encoding categorical variables, normalizing numeric data, and visualizing the distribution of a specific column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ffa3fc-7bfa-4500-948d-10a383500a94",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5942a2-ce1e-4b82-9488-59da954262ae",
   "metadata": {},
   "source": [
    "##### 1. Linear Regression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717b734-80bd-430d-9695-f0039262c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "data_encoded = pd.get_dummies(data)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = imputer.fit_transform(data_encoded)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data_encoded.columns)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data_imputed.drop(columns=['total_emission'])\n",
    "y = data_imputed['total_emission']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e89d1f-2b37-4c7f-bf73-97d5383ae7da",
   "metadata": {},
   "source": [
    "The results of your regression model evaluation indicate the following:\n",
    "\n",
    "Mean Squared Error (MSE): 125919.21\n",
    "\n",
    "The MSE is relatively high, suggesting that the model's predictions have some error, but this needs to be interpreted in the context of your data's scale.\n",
    "\n",
    "R-squared (R²): 0.999998\n",
    "\n",
    "This metric indicates how well the model's predictions match the actual data. An R² value close to 1 means that the model explains almost all the variability in the target variable. R² value of 0.999998 is extremely high, indicating that the model fits the data very well.\n",
    "\n",
    "In summary, while the MSE suggests there is some error in the predictions, the very high R² value indicates that the model is highly accurate in explaining the variance in the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5d229-ad39-4d3e-ac17-51d82e81f7f7",
   "metadata": {},
   "source": [
    "##### 2. Cross-Validation Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e813a68f-3e5c-4c52-b94d-738a5d595a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['total_emission'])\n",
    "y = data['total_emission']\n",
    "\n",
    "# Define a preprocessing pipeline for numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that includes preprocessing and the regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "mean_mse = mse_scores.mean()\n",
    "std_mse = mse_scores.std()\n",
    "\n",
    "print(f\"Mean Squared Error (cross-validated): {mean_mse}\")\n",
    "print(f\"Standard Deviation of MSE: {std_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac95f44-0eef-4356-84f3-3f668bb1a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['total_emission'])\n",
    "y = data['total_emission']\n",
    "\n",
    "# Define a preprocessing pipeline for numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that includes preprocessing and the regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Define the K-Fold cross-validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "mean_mse = mse_scores.mean()\n",
    "std_mse = mse_scores.std()\n",
    "\n",
    "print(f\"Mean Squared Error (cross-validated): {mean_mse}\")\n",
    "print(f\"Standard Deviation of MSE: {std_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ca31e-7787-4d9c-9c6e-18fdc5731a80",
   "metadata": {},
   "source": [
    "The output of your K-Fold cross-validation provides insights into the performance and stability of your regression model:\n",
    "\n",
    "Mean Squared Error (cross-validated): 44,822,329.39\n",
    "\n",
    "This value represents the average squared difference between the actual and predicted values across all cross-validation folds. A higher MSE indicates larger errors in the model's predictions. In your case, the MSE is quite large, suggesting that the model's predictions have significant errors. This could be due to various factors such as the scale of the target variable or potential issues with the model or data.\n",
    "Standard Deviation of MSE: 2,617,200.09\n",
    "\n",
    "This value measures the variability of the MSE scores across the different cross-validation folds. A lower standard deviation indicates that the model's performance is more consistent across different subsets of the data. In your case, the standard deviation is relatively low compared to the mean MSE, suggesting that while the model's errors are large, its performance is relatively stable across different folds.\n",
    "In summary, while the model's average error is quite high, the relatively low standard deviation indicates that the model's performance is consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eadc35-038b-4fa1-a2aa-64a96e2984be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming y_test and y_pred are your actual and predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e60fe-de54-4b9e-8426-25289cd1c936",
   "metadata": {},
   "source": [
    "Scatter Plot of Actual vs. Predicted Values\n",
    "Purpose: To compare the actual values with the predicted values.\n",
    "Interpretation:\n",
    "Points close to the diagonal line indicate good predictions.\n",
    "Points far from the line indicate errors.\n",
    "A tight cluster around the line suggests a well-performing model.\n",
    "If the points are randomly scattered around the line, it indicates that the model is capturing the underlying pattern well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d3581-741e-49d6-8187-b75e92e21d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='r', linestyles='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb1220-4698-4dd8-b065-e472f766c248",
   "metadata": {},
   "source": [
    "Residual Plot\n",
    "Purpose: To visualize the residuals (errors) of the predictions.\n",
    "Interpretation:\n",
    "Residuals should be randomly distributed around zero.\n",
    "A random pattern suggests that the model's assumptions are valid.\n",
    "Patterns or trends in the residuals (e.g., a funnel shape) may indicate issues like heteroscedasticity or non-linearity.\n",
    "Large residuals indicate points where the model's predictions are significantly off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f0e35-f8ea-40b2-8204-75da3b5a5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "train_scores_mean = -train_scores.mean(axis=1)\n",
    "test_scores_mean = -test_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6e4e9-78b9-4ec2-8d94-5f865ac750d8",
   "metadata": {},
   "source": [
    "The above visualization is a leaning Curve\n",
    "Purpose: To evaluate the model's performance as a function of the training set size.\n",
    "Interpretation:\n",
    "Training Score: The model's performance on the training data.\n",
    "Validation Score: The model's performance on the validation data.\n",
    "Overfitting: If the training score is high and the validation score is low, the model is overfitting.\n",
    "Underfitting: If both the training and validation scores are low, the model is underfitting.\n",
    "Good Fit: If both scores are high and close to each other, the model is well-fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6c471-c6ab-4965-bcf0-6fb671ff0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "data_encoded = pd.get_dummies(data)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = imputer.fit_transform(data_encoded)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data_encoded.columns)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data_imputed.drop(columns=['total_emission'])\n",
    "y = data_imputed['total_emission']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "dt_y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "dt_mse = mean_squared_error(y_test, dt_y_pred)\n",
    "dt_r2 = r2_score(y_test, dt_y_pred)\n",
    "\n",
    "print(f\"Decision Tree - Mean Squared Error: {dt_mse}\")\n",
    "print(f\"Decision Tree - R-squared: {dt_r2}\")\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
    "rf_r2 = r2_score(y_test, rf_y_pred)\n",
    "\n",
    "print(f\"Random Forest - Mean Squared Error: {rf_mse}\")\n",
    "print(f\"Random Forest - R-squared: {rf_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63363c9-749f-4b4f-aa7c-9a47c93e45c0",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE):\n",
    "\n",
    "This metric measures the average squared difference between the actual and predicted values. A lower MSE indicates a better fit of the model to the data.\n",
    "For the Decision Tree, the MSE is 783,725,802.21.\n",
    "For the Random Forest, the MSE is 634,854,371.41.\n",
    "The Random Forest has a lower MSE, indicating it has a better fit to the data compared to the Decision Tree.\n",
    "\n",
    "R-squared (R²):\n",
    "\n",
    "This metric represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with higher values indicating a better fit.\n",
    "For the Decision Tree, the R² is 0.9887.\n",
    "For the Random Forest, the R² is 0.9908.\n",
    "Both models have high R² values, indicating they explain a large portion of the variance in the target variable. The Random Forest has a slightly higher R², suggesting it performs better overall.\n",
    "In summary, both models perform well, but the Random Forest model has a slight edge over the Decision Tree in terms of both MSE and R². This is expected as Random Forests are generally more robust and less prone to overfitting compared to single Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a68019-434b-4bf6-9a4b-7fc6438e3fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "data_encoded = pd.get_dummies(data)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = imputer.fit_transform(data_encoded)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data_encoded.columns)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data_imputed.drop(columns=['total_emission'])\n",
    "y = data_imputed['total_emission']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Support Vector Regressor model\n",
    "svr_model = SVR()\n",
    "# Train the model\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "svr_y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "svr_mse = mean_squared_error(y_test, svr_y_pred)\n",
    "svr_r2 = r2_score(y_test, svr_y_pred)\n",
    "\n",
    "print(f\"Support Vector Regressor - Mean Squared Error: {svr_mse}\")\n",
    "print(f\"Support Vector Regressor - R-squared: {svr_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bc30e-c8bf-48ac-86d1-5b657f917931",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e65b4-7b5b-421a-9577-53fb9c1c4512",
   "metadata": {},
   "source": [
    "In regards to the models created we have demonstrated the relationships between the variables and how one variables impacts another.\n",
    "When reviewing the C02 emissions data it is clear to us that in order to make informed predictions we have to consider variables that have a strong correllation, in doing so we have established that strong correlations exist between the following variables:\n",
    " - with total_emission(y)\n",
    " - (x) variables could be\n",
    "     - urban_population\n",
    "     - food_household_consumption\n",
    "     - IPPU\n",
    "     - food_packaging\n",
    "When analysing the data it also clear that in areas where food production has a higher demand due the urban population size, this impacts the CO2 emissions e.g. higher population = higher emissions.\n",
    "\n",
    "The evaluation compares regression models:\n",
    "• Linear Regression has a very high R² of 0.999998, meaning it explains almost all variance, but its high Mean Squared Error (MSE) indicates some prediction error. When validated with K-Fold, it shows a much higher MSE, suggesting possible overfitting.\n",
    "• Decision Tree shows an MSE of ~783 million and R² of 0.9887.\n",
    "• Random Forest performs best with a lower MSE of ~634 million and higher R² of 0.9908, indicating more accurate and robust results due to its ensemble nature.\n",
    "Overall Random Forest is the best model overall due to its balance between accuracy and generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
