{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59554160-394c-44e8-80eb-fadadc0db60c",
   "metadata": {},
   "source": [
    "# Regression Group Project CO2 Emissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a754712-9116-4efb-a6a2-e7386f992ec3",
   "metadata": {},
   "source": [
    "<img src=\"https://images.propertycasualty360.com/contrib/content/uploads/sites/414/2022/04/Pollution-Solution-Image_0422-NU.jpg\" alt=\"Pollution Solution\" width=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e89cd7d-74c2-4a00-9d9e-e2b0c0d733f4",
   "metadata": {},
   "source": [
    "## Team Members\n",
    "##### Khuthadzo Tshikovhele\n",
    "##### Dembe Tsiwana\n",
    "##### Keamogetswe Peterson\n",
    "##### Aston Greeves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60dd22a-e997-44b0-9f96-f1a205e236ec",
   "metadata": {},
   "source": [
    "## Loading of Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46a32e-2482-4e95-9788-505cb1b9219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78354735-2c55-4367-9f4f-a50e2e1a1a62",
   "metadata": {},
   "source": [
    "## Data Collection and Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af7aa45-f54e-420b-9826-47bf1070c339",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('co2_emissions_from_agri.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e4ced-83d1-42da-afe1-620a8c31db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the dataframe\n",
    "print(\"Initial data:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f077978-6ed0-4e1d-bb5e-d8901fa21954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the column names of the dataframe\n",
    "print(\"\\nColumn names in the dataframe:\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9692b-af6c-45b5-8280-ca2b9e99004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with missing values\n",
    "data_cleaned = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715692c7-dbe4-4a82-9bea-1719ed2398da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows of the cleaned dataframe\n",
    "print(\"\\nData after dropping rows with missing values:\")\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409eea53-6304-4681-a008-2a32344a16d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'CO2_Emissions' is in the columns\n",
    "if 'total_emission' in data_cleaned.columns:\n",
    "    # Filter the data for a specific condition (e.g., CO2 emissions greater than a certain value)\n",
    "    filtered_data = data_cleaned[data_cleaned['total_emission'] > 1000]\n",
    "       \n",
    "# Display the first few rows of the filtered dataframe\n",
    "    print(\"\\nFiltered data (total_emission > 1000):\")\n",
    "    print(filtered_data.head())\n",
    "\n",
    "# Save the cleaned and filtered data to a new CSV file\n",
    "    filtered_data.to_csv('cleaned_filtered_co2_emissions.csv', index=False)\n",
    "\n",
    "    print(\"\\nCleaned and filtered data saved to 'cleaned_filtered_co2_emissions.csv'\")\n",
    "else:\n",
    "    print(\"\\nColumn 'CO2_Emissions' not found in the dataframe. Please check the column names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a40387-22b6-4e2b-8b32-2c9fcd6a45d1",
   "metadata": {},
   "source": [
    "**Handling Duplicate rows and removing them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a424753-0f38-493e-b927-d90583b401b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "data_cleaned = data_cleaned.drop_duplicates()\n",
    "\n",
    "# Display the first few rows of the dataframe after removing duplicates\n",
    "print(\"\\nData after removing duplicate rows:\")\n",
    "print(data_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d243158-0bb1-48ff-913d-a9988bb4ce54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data types if necessary (e.g., converting a column to numeric)\n",
    "# Assuming 'Year' column exists and needs to be converted to integer\n",
    "if 'Year' in data_cleaned.columns:\n",
    "    data_cleaned['Year'] = pd.to_numeric(data_cleaned['Year'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9979e3-c8f0-4952-8b13-b544795a7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data types of the dataframe\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(data_cleaned.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57e6cf-1898-4a91-b162-0ed7bbf0aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for consistency (e.g., converting column names to lowercase)\n",
    "data_cleaned.columns = [col.lower() for col in data_cleaned.columns]\n",
    "\n",
    "# Display the column names after renaming\n",
    "print(\"\\nColumn names after renaming:\")\n",
    "print(data_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea55e03f-2825-403a-9e51-13f75cce9b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new CSV file\n",
    "data_cleaned.to_csv('cleaned_co2_emissions.csv', index=False)\n",
    "\n",
    "print(\"\\nCleaned data saved to 'cleaned_co2_emissions.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c52418-c36e-4287-98ad-78062eb0e2e2",
   "metadata": {},
   "source": [
    "**Handling Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65b97a-0754-43e7-aa2d-fad98086fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for outlier removal\n",
    "numeric_data = data.select_dtypes(include=['number'])\n",
    "\n",
    "# Remove outliers using the IQR method\n",
    "Q1 = numeric_data.quantile(0.25)\n",
    "Q3 = numeric_data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Filter out the outliers\n",
    "data_no_outliers = numeric_data[~((numeric_data < (Q1 - 1.5 * IQR)) | (numeric_data > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Display the first few rows of the dataframe after removing outliers\n",
    "print(\"\\nData after removing outliers:\")\n",
    "print(data_no_outliers.head())\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "data_no_outliers.to_csv('data_no_outliers.csv', index=False)\n",
    "\n",
    "print(\"\\nData without outliers saved to 'data_no_outliers.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbad77d-53ab-41a8-9703-4638b1a62c81",
   "metadata": {},
   "source": [
    "**Inputing Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee716e2b-7723-4c49-8747-92890c8df650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the mean of the numeric columns\n",
    "data_filled = data.fillna(data.mean(numeric_only=True))\n",
    "\n",
    "# Display the first few rows of the dataframe after filling missing values\n",
    "print(\"\\nData after filling missing values with mean:\")\n",
    "print(data_filled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8862cef-dd4f-4e18-be9c-8938befa8052",
   "metadata": {},
   "source": [
    "**Normalizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7869224-2ef0-4c6f-940d-714f56f1a83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data provided by the user\n",
    "data_dict = {\n",
    "    \"Area\": [\"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\"],\n",
    "    \"Year\": [1990, 1991, 1992, 1993, 1994],\n",
    "    \"Savanna fires\": [14.7237, 14.7237, 14.7237, 14.7237, 14.7237],\n",
    "    \"Forest fires\": [0.0557, 0.0557, 0.0557, 0.0557, 0.0557],\n",
    "    \"Crop Residues\": [205.6077, 209.4971, 196.5341, 230.8175, 242.0494],\n",
    "    \"Rice Cultivation\": [686.00, 678.16, 686.00, 686.00, 705.60],\n",
    "    \"Drained organic soils (CO2)\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"Pesticides Manufacturing\": [11.807483, 11.712073, 11.712073, 11.712073, 11.712073],\n",
    "    \"Food Transport\": [63.1152, 61.2125, 53.3170, 54.3617, 53.9874],\n",
    "    \"Forestland\": [-2388.803, -2388.803, -2388.803, -2388.803, -2388.803],\n",
    "    \"Manure Management\": [319.1763, 342.3079, 349.1224, 352.2947, 367.6784],\n",
    "    \"Fires in organic soils\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"Fires in humid tropical forests\": [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"On-farm energy use\": [None] * 5,\n",
    "    \"Rural population\": [9655167, 10230490, 10995568, 11858090, 12690115],\n",
    "    \"Urban population\": [2593947, 2763167, 2985663, 3237009, 3482604],\n",
    "    \"Total Population - Male\": [5348387.0, 5372959.0, 6028494.0, 7003641.0, 7733458.0],\n",
    "    \"Total Population - Female\": [5346409.0, 5372208.0, 6028939.0, 7000119.0, 7722096.0],\n",
    "    \"total_emission\": [2198.963539, 2323.876629, 2356.304229, 2368.470529, 2500.768729],\n",
    "    \"Average Temperature °C\": [0.536167, 0.020667, -0.259583, 0.101917, 0.372250]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "# Display the initial data\n",
    "print(\"Initial data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Fill missing values with the mean of the numeric columns\n",
    "data_filled = data.fillna(data.mean(numeric_only=True))\n",
    "\n",
    "# Normalize the numeric columns\n",
    "numeric_columns = data_filled.select_dtypes(include=['float64', 'int64']).columns\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = pd.DataFrame(scaler.fit_transform(data_filled[numeric_columns]), columns=numeric_columns)\n",
    "\n",
    "# Combine the normalized numeric columns with the non-numeric columns\n",
    "data_normalized = pd.concat([data_filled.select_dtypes(exclude=['float64', 'int64']).reset_index(drop=True), data_normalized], axis=1)\n",
    "\n",
    "# Display the first few rows of the normalized dataframe\n",
    "print(\"\\nNormalized data:\")\n",
    "print(data_normalized.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14013e00-261a-48df-a781-38e077ab66c5",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97102a1-5849-48ea-83c5-aa7cc76643ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data['total_emission'], kde=True)\n",
    "plt.title('Distribution of Total Emissions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7590d1-b105-4899-9c52-7e539c50b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot\n",
    "sns.pairplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba403ba7-9e1d-403a-ae65-5cee0f984ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Plotly Express\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('co2_emissions_from_agri.csv')\n",
    "\n",
    "# Scatter plot\n",
    "fig = px.scatter(data, x='Year', y='total_emission', color='Area')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb092f-10ae-4f61-b875-c6c0760372ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('co2_emissions_from_agri.csv')\n",
    "\n",
    "# Generate some data\n",
    "data = np.random.rand(10, 12)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(12, 8))  # Adjust the width and height as needed\n",
    "sns.heatmap(data, annot=True, cmap='viridis')\n",
    "\n",
    "# Display the heatmap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c374c54-689d-4c86-97b0-64426d13dddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv('co2_emissions_from_agri.csv')\n",
    "\n",
    "# Group the data by 'Area' and calculate the total emissions for each area\n",
    "total_emissions_by_area = data.groupby('Area')['total_emission'].sum()\n",
    "\n",
    "# Get the top 10 and bottom 10 areas with the highest and lowest emissions\n",
    "top10_emissions_areas = total_emissions_by_area.nlargest(10)\n",
    "bottom10_emissions_areas = total_emissions_by_area.nsmallest(10)\n",
    "\n",
    "# Plot the top 10 areas with the highest emissions as a horizontal bar graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "top10_emissions_areas.plot(kind='barh', color='blue')\n",
    "plt.title('Areas with the Highest CO2 Emissions')\n",
    "plt.xlabel('Total Emissions')\n",
    "plt.ylabel('Area')\n",
    "plt.show()\n",
    "\n",
    "# Plot the bottom 10 areas with the lowest emissions as a horizontal bar graph\n",
    "plt.figure(figsize=(12, 6))\n",
    "bottom10_emissions_areas.plot(kind='barh', color='green')\n",
    "plt.title('Areas with the Lowest CO2 Emissions')\n",
    "plt.xlabel('Total Emissions')\n",
    "plt.ylabel('Area')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be88cf9d-402c-4419-a8c3-23f4b657f069",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd802d3-31df-48ce-b204-564c3b8b1a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('co2_emissions_from_agri.csv')\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Fill missing values with the mean of the column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# One-hot encode the 'Area' column\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_area = encoder.fit_transform(df[['Area']])\n",
    "\n",
    "# Create a DataFrame with the encoded 'Area' column\n",
    "encoded_area_df = pd.DataFrame(encoded_area, columns=encoder.get_feature_names_out(['Area']))\n",
    "\n",
    "# Concatenate the encoded 'Area' DataFrame with the original DataFrame\n",
    "df_encoded = pd.concat([df.drop(columns=['Area']), encoded_area_df], axis=1)\n",
    "\n",
    "# Ensure all columns are numeric\n",
    "df_encoded = df_encoded.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Display the first few rows of the encoded dataframe\n",
    "print(df_encoded.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717b734-80bd-430d-9695-f0039262c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "data_encoded = pd.get_dummies(data)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = imputer.fit_transform(data_encoded)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data_encoded.columns)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data_imputed.drop(columns=['total_emission'])\n",
    "y = data_imputed['total_emission']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e89d1f-2b37-4c7f-bf73-97d5383ae7da",
   "metadata": {},
   "source": [
    "This code uses a pipeline to handle preprocessing (imputing missing values and encoding categorical variables) and the regression model. It then evaluates the model using 5-fold cross-validation, providing a mean and standard deviation of the mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e813a68f-3e5c-4c52-b94d-738a5d595a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "# Separate features and target variable\n",
    "X = data.drop(columns=['total_emission'])\n",
    "y = data['total_emission']\n",
    "\n",
    "# Define a preprocessing pipeline for numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Create a pipeline that includes preprocessing and the regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "mse_scores = -scores\n",
    "mean_mse = mse_scores.mean()\n",
    "std_mse = mse_scores.std()\n",
    "\n",
    "print(f\"Mean Squared Error (cross-validated): {mean_mse}\")\n",
    "print(f\"Standard Deviation of MSE: {std_mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac95f44-0eef-4356-84f3-3f668bb1a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Assuming y_test and y_pred are your actual and predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "plt.xlabel('Actual')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821ca31e-7787-4d9c-9c6e-18fdc5731a80",
   "metadata": {},
   "source": [
    "Scatter Plot of Actual vs. Predicted Values\n",
    "Purpose: To compare the actual values with the predicted values.\n",
    "Interpretation:\n",
    "Points close to the diagonal line indicate good predictions.\n",
    "Points far from the line indicate errors.\n",
    "A tight cluster around the line suggests a well-performing model.\n",
    "If the points are randomly scattered around the line, it indicates that the model is capturing the underlying pattern well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eadc35-038b-4fa1-a2aa-64a96e2984be",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.7)\n",
    "plt.hlines(y=0, xmin=y_pred.min(), xmax=y_pred.max(), colors='r', linestyles='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e60fe-de54-4b9e-8426-25289cd1c936",
   "metadata": {},
   "source": [
    "Residual Plot\n",
    "Purpose: To visualize the residuals (errors) of the predictions.\n",
    "Interpretation:\n",
    "Residuals should be randomly distributed around zero.\n",
    "A random pattern suggests that the model's assumptions are valid.\n",
    "Patterns or trends in the residuals (e.g., a funnel shape) may indicate issues like heteroscedasticity or non-linearity.\n",
    "Large residuals indicate points where the model's predictions are significantly off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d3581-741e-49d6-8187-b75e92e21d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "train_scores_mean = -train_scores.mean(axis=1)\n",
    "test_scores_mean = -test_scores.mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color='g', label='Cross-validation score')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb1220-4698-4dd8-b065-e472f766c248",
   "metadata": {},
   "source": [
    "The above visualization is a leaning Curve\n",
    "Purpose: To evaluate the model's performance as a function of the training set size.\n",
    "Interpretation:\n",
    "Training Score: The model's performance on the training data.\n",
    "Validation Score: The model's performance on the validation data.\n",
    "Overfitting: If the training score is high and the validation score is low, the model is overfitting.\n",
    "Underfitting: If both the training and validation scores are low, the model is underfitting.\n",
    "Good Fit: If both scores are high and close to each other, the model is well-fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f0e35-f8ea-40b2-8204-75da3b5a5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "data_encoded = pd.get_dummies(data)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = imputer.fit_transform(data_encoded)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data_encoded.columns)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data_imputed.drop(columns=['total_emission'])\n",
    "y = data_imputed['total_emission']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Decision Tree model\n",
    "dt_model = DecisionTreeRegressor(random_state=42)\n",
    "# Train the model\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "dt_y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "dt_mse = mean_squared_error(y_test, dt_y_pred)\n",
    "dt_r2 = r2_score(y_test, dt_y_pred)\n",
    "\n",
    "print(f\"Decision Tree - Mean Squared Error: {dt_mse}\")\n",
    "print(f\"Decision Tree - R-squared: {dt_r2}\")\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_mse = mean_squared_error(y_test, rf_y_pred)\n",
    "rf_r2 = r2_score(y_test, rf_y_pred)\n",
    "\n",
    "print(f\"Random Forest - Mean Squared Error: {rf_mse}\")\n",
    "print(f\"Random Forest - R-squared: {rf_r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d6e4e9-78b9-4ec2-8d94-5f865ac750d8",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE):\n",
    "\n",
    "This metric measures the average squared difference between the actual and predicted values. A lower MSE indicates a better fit of the model to the data.\n",
    "For the Decision Tree, the MSE is 783,725,802.21.\n",
    "For the Random Forest, the MSE is 634,854,371.41.\n",
    "The Random Forest has a lower MSE, indicating it has a better fit to the data compared to the Decision Tree.\n",
    "\n",
    "R-squared (R²):\n",
    "\n",
    "This metric represents the proportion of the variance in the dependent variable that is predictable from the independent variables. It ranges from 0 to 1, with higher values indicating a better fit.\n",
    "For the Decision Tree, the R² is 0.9887.\n",
    "For the Random Forest, the R² is 0.9908.\n",
    "Both models have high R² values, indicating they explain a large portion of the variance in the target variable. The Random Forest has a slightly higher R², suggesting it performs better overall.\n",
    "In summary, both models perform well, but the Random Forest model has a slight edge over the Decision Tree in terms of both MSE and R². This is expected as Random Forests are generally more robust and less prone to overfitting compared to single Decision Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6c471-c6ab-4965-bcf0-6fb671ff0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Assuming data is your DataFrame\n",
    "# Convert categorical variables to numeric using one-hot encoding\n",
    "data_encoded = pd.get_dummies(data)\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = imputer.fit_transform(data_encoded)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data_encoded.columns)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data_imputed.drop(columns=['total_emission'])\n",
    "y = data_imputed['total_emission']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Support Vector Regressor model\n",
    "svr_model = SVR()\n",
    "# Train the model\n",
    "svr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "svr_y_pred = svr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "svr_mse = mean_squared_error(y_test, svr_y_pred)\n",
    "svr_r2 = r2_score(y_test, svr_y_pred)\n",
    "\n",
    "print(f\"Support Vector Regressor - Mean Squared Error: {svr_mse}\")\n",
    "print(f\"Support Vector Regressor - R-squared: {svr_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f7d88-6910-4d2a-96a8-d7a362e2804c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
